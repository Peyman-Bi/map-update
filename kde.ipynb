{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random, string\n",
    "from scipy.spatial.distance import cdist\n",
    "import geopandas as gp\n",
    "from itertools import tee\n",
    "from shapely import wkb\n",
    "import os, sys\n",
    "from time import sleep\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARTH_RADIUS = 6371000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_in_meters(node1, node2):\n",
    "    METERS_PER_LONGITUDE = 1000\n",
    "    METERS_PER_LATITUDE = 1000\n",
    "    distance = np.sqrt(\n",
    "        ((node1.coordinates[0]-node2.coordinates[0])*METERS_PER_LONGITUDE)**2+\n",
    "        ((node1.coordinates[1]-node2.coordinates[1])*METERS_PER_LATITUDE)**2\n",
    "    )\n",
    "    return distnace\n",
    "\n",
    "def same_coords(a_lat, a_lon, b_lat, b_lon):\n",
    "    if (a_lat == b_lat and a_lon == b_lon):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def haversine_distance(coordinates1, coordinates2):\n",
    "    (a_lat, a_lon) = coordinates1\n",
    "    (b_lat, b_lon) = coordinates2\n",
    "    if same_coords(a_lat, a_lon, b_lat, b_lon):\n",
    "        return 0.0\n",
    "    \n",
    "    dLat = math.radians(b_lat - a_lat)\n",
    "    dLon = math.radians(b_lon - a_lon)\n",
    "    \n",
    "    a = math.sin(dLat/2.0) * math.sin(dLat/2.0) + math.cos(math.radians(a_lat)) * math.cos(math.radians(b_lat)) * math.sin(dLon/2.0) * math.sin(dLon/2.0)\n",
    "    \n",
    "    c = 2.0 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = EARTH_RADIUS * c\n",
    "    \n",
    "    return d\n",
    "\n",
    "def point_along_line(coordinates1, coordinates2, fraction_along):\n",
    "    c_lat = coordinates1[0] + (fraction_along * (coordinates2[0] - coordinates1[0]))\n",
    "    c_lon = coordinates1[1] + (fraction_along * (coordinates2[1] - coordinates1[1]))\n",
    "    return c_lat, c_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, node_id, latitude, longitude):\n",
    "        self.id = node_id\n",
    "        self.coordinates = (latitude, longitude)\n",
    "        self.edges = set()\n",
    "        self.neighbors = set()\n",
    "\n",
    "\n",
    "def distance_argmin(nodes, sample_node):\n",
    "    distances = []\n",
    "    for i in range(len(nodes)):\n",
    "        distances.append(haversine_distance(nodes[i], sample_node))\n",
    "    return np.argmin(distances)\n",
    "\n",
    "        \n",
    "def stick_edge(point_edge_list, nodes_list):\n",
    "    big_edges = {}\n",
    "    while len(point_edge_list) > 0:\n",
    "        point_key, edge_ids = point_edge_list.popitem()\n",
    "        edge_ids = list(edge_ids)\n",
    "        for edge_id in edge_ids:\n",
    "            for point, edges in point_edge_list.copy().items():\n",
    "                if edge_id in edges:\n",
    "                    _ = point_edge_list.pop(point)\n",
    "                    edge_ids += list(edges.difference(set(edge_ids)))\n",
    "        big_edges[point_key] = set(edge_ids)\n",
    "    return big_edges\n",
    "\n",
    "\n",
    "def generate_node_mapping(coords_geoms):\n",
    "    nodes_str_mapping = {}\n",
    "    point_edges = {}\n",
    "    id_counter = 0\n",
    "    for index, item in coords_geoms.iteritems():\n",
    "        for i in range(item.shape[0]):\n",
    "            node_key = f'({item[i][0]},{item[i][1]})'\n",
    "            if node_key not in nodes_str_mapping.keys():\n",
    "                nodes_str_mapping[node_key] = Node(id_counter, *item[i])\n",
    "                nodes_str_mapping[node_key].edges.add(index)\n",
    "                id_counter += 1\n",
    "            else:\n",
    "                nodes_str_mapping[node_key].edges.add(index)\n",
    "\n",
    "    nodes_mapping = {}\n",
    "    for index, item in coords_geoms.iteritems():\n",
    "        for i in range(item.shape[0]):\n",
    "            node_key = f'({item[i][0]},{item[i][1]})'\n",
    "            node_id = nodes_str_mapping[node_key].id\n",
    "            nodes_mapping[node_id] = nodes_str_mapping[node_key]\n",
    "            nodes_mapping[node_id].neighbors.add(\n",
    "                nodes_str_mapping[f'({item[int(not(i))][0]},{item[int(not(i))][1]})'].id\n",
    "            )\n",
    "    return nodes_mapping\n",
    "\n",
    "\n",
    "def generate_holes(nodes_mapping, r, d, check_threshold):\n",
    "    initial_mapping_len = len(nodes_mapping)\n",
    "    num_checked_nodes = 0\n",
    "    # fixme: break out change the condition\n",
    "    holes = set()\n",
    "    while num_checked_nodes/initial_mapping_len < check_threshold:\n",
    "        temp_holes = []\n",
    "        traveled_distance = 0\n",
    "        random_node_id = np.random.choice(np.array(list(nodes_mapping.keys())), 1)[0]\n",
    "        temp_holes.append(nodes_mapping[random_node_id].coordinates)\n",
    "        recursive_hole_generation(deepcopy(nodes_mapping), r, d, random_node_id, temp_holes, traveled_distance)\n",
    "        random_neighbor_ids = nodes_mapping.pop(random_node_id).neighbors\n",
    "        for neighbor_id in random_neighbor_ids:\n",
    "            nodes_mapping[neighbor_id].neighbors.discard(random_node_id)\n",
    "        holes = holes.union(set(temp_holes))\n",
    "        num_checked_nodes += len(random_neighbor_ids)+1\n",
    "    return holes\n",
    "    \n",
    "    \n",
    "def recursive_node_generation(nodes_mapping, r, d, init_node_id, holes, traveled_distance):\n",
    "    init_node = nodes_mapping[init_node_id]\n",
    "    for neighbor_id in init_node.neighbors.copy():\n",
    "        distance = haversine_distance(init_node.coordinates, nodes_mapping[neighbor_id].coordinates)\n",
    "        if distance > d and traveled_distance+d < r:\n",
    "            node_cooridiants = point_along_line(init_node.coordinates, nodes_mapping[neighbor_id].coordinates, d/distance)\n",
    "            new_id = len(nodes_mapping)\n",
    "            holes.append(node_cooridiants)\n",
    "            nodes_mapping[new_id] = Node(new_id, *node_cooridiants)\n",
    "            nodes_mapping[new_id].neighbors.add(neighbor_id)\n",
    "            nodes_mapping[init_node_id].neighbors.discard(neighbor_id)\n",
    "            nodes_mapping[neighbor_id].neighbors.discard(init_node_id)\n",
    "            recursive_node_generation(nodes_mapping, r, d, new_id, holes, traveled_distance+d)\n",
    "        elif distance < d and traveled_distance+distance < r:\n",
    "            holes.append(nodes_mapping[neighbor_id].coordinates)\n",
    "            nodes_mapping[neighbor_id].neighbors.discard(init_node_id)\n",
    "            recursive_node_generation(nodes_mapping, r, d, neighbor_id, holes, traveled_distance+distance)\n",
    "            \n",
    "            \n",
    "def generate_nodes(gt_nodes_mapping, pr_nodes_mapping, r, d, check_threshold):\n",
    "    initial_mapping_len = len(gt_nodes_mapping)\n",
    "    num_checked_nodes = 0\n",
    "    # fixme: change the break out condition\n",
    "    holes = set()\n",
    "    new_nodes = {'holes':set(), 'marbles':set()}\n",
    "    while num_checked_nodes/initial_mapping_len < check_threshold:\n",
    "        itr_dict = {'holes':gt_nodes_mapping, 'marbles':pr_nodes_mapping}\n",
    "        temp_gen_nodes = {'holes':[], 'marbles':[]}\n",
    "        for key, nodes_mapping in itr_dict.items():\n",
    "            traveled_dist = 0\n",
    "            if key == 'holes':\n",
    "                random_node_id = np.random.choice(np.array(list(nodes_mapping.keys())), 1)[0]\n",
    "                random_hole_id, last_gt_mappings = random_node_id, deepcopy(nodes_mapping)\n",
    "                num_checked_nodes += len(nodes_mapping[random_node_id].neighbors)+1\n",
    "            else:\n",
    "                pr_nodes = list(nodes_mapping.values())\n",
    "                pr_nodes_coords = np.array([node.coordinates for node in pr_nodes])\n",
    "                try:\n",
    "                    random_node_id = pr_nodes[distance_argmin(pr_nodes_coords, last_gt_mappings[random_hole_id].coordinates)].id\n",
    "                except:\n",
    "                    print(f'Warnnings!\\nThere are not not enough predicted nodes on map to check'\n",
    "                          f' more at least {check_threshold*100}% of ground truth nodes.')\n",
    "            temp_gen_nodes[key].append(nodes_mapping[random_node_id].coordinates)\n",
    "            recursive_node_generation(deepcopy(nodes_mapping), r, d, random_node_id, temp_gen_nodes[key], traveled_dist)\n",
    "            random_nbr_ids = nodes_mapping.pop(random_node_id).neighbors\n",
    "            for neighbor_id in random_nbr_ids:\n",
    "                nodes_mapping[neighbor_id].neighbors.discard(random_node_id)\n",
    "            new_nodes[key] = new_nodes[key].union(set(temp_gen_nodes[key]))\n",
    "    for key, value in new_nodes.items():\n",
    "        new_nodes[key] = list(value)\n",
    "    return new_nodes\n",
    "\n",
    "def match_hole_marble(generated_nodes, error_threshold):\n",
    "    MAX_VALUE = 1e05\n",
    "    distances = np.zeros((len(generated_nodes['marbles']), len(generated_nodes['holes'])))\n",
    "    for i in range(distances.shape[0]):\n",
    "        for j in range(distances.shape[1]):\n",
    "            distances[i, j] = haversine_distance(generated_nodes['marbles'][i], generated_nodes['holes'][j])\n",
    "    matches = []\n",
    "    for i in range(distances.shape[0]):\n",
    "        nearest_hole_index = np.argmin(distances[i])\n",
    "        for j in range(distances.shape[1]):\n",
    "            nearest_marble_index = np.argmin(distances[:, j])\n",
    "            if nearest_hole_index == j and nearest_marble_index == i:\n",
    "                if distances[i, j] <= error_threshold:\n",
    "                    matches.append((i, j))\n",
    "                distances[i] = MAX_VALUE\n",
    "                distances[:, j] = MAX_VALUE\n",
    "    return matches, distances\n",
    "        \n",
    "\n",
    "def evalute_prf(generated_nodes, error_threshold):\n",
    "    matches, _ = match_hole_marble(generated_nodes, error_threshold)\n",
    "    precision = len(matches)/len(generated_nodes['marbles'])\n",
    "    recall = len(matches)/len(generated_nodes['holes'])\n",
    "    f_score = 2*precision*recall/(precision+recall)\n",
    "    return precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = 'ground-map/map/filtered_edges.shp'\n",
    "ground_map = gp.read_file(ground_truth_path)\n",
    "gt_edges = ground_map['geometry'].apply(lambda x: np.array(x.coords)[:, ::-1])\n",
    "con = sqlite3.connect(\"./gis12_mapinference/skeleton_maps/skeleton_map_1m.db\")\n",
    "edges_df = pd.read_sql_query(\"SELECT id, in_node, out_node, weight FROM edges\", con)\n",
    "nodes_df = pd.read_sql_query(\"SELECT id, latitude, longitude, weight FROM nodes\", con)\n",
    "nodes_df.set_index('id', drop=True, inplace=True)\n",
    "pr_edges = edges_df.apply(lambda x: np.array([\n",
    "    [nodes_df.loc[x.in_node].latitude, nodes_df.loc[x.in_node].longitude],\n",
    "    [nodes_df.loc[x.out_node].latitude, nodes_df.loc[x.out_node].longitude]\n",
    "]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, d, check_threshold, error_threshold = 1000, 100, 0.15, 50\n",
    "gt_nodes_dict = generate_node_mapping(gt_edges)\n",
    "pr_nodes_dict = generate_node_mapping(pr_edges)\n",
    "new_gen_nodes = generate_nodes(gt_nodes_dict, pr_nodes_dict, r, d, check_threshold)\n",
    "precision, recall, f_score = evalute_prf(new_gen_nodes, error_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-023bc3b5a019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/peyman/Downloads/part-00000-f43ea5ba-b3fe-450a-9478-8803ecab3b4b-c000.snappy.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size)\u001b[0m\n\u001b[1;32m    196\u001b[0m                  read_dictionary=None, memory_map=False, buffer_size=0):\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         self.reader.open(source, use_memory_map=memory_map,\n\u001b[0m\u001b[1;32m    199\u001b[0m                          \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                          read_dictionary=read_dictionary, metadata=metadata)\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36mpyarrow._parquet.ParquetReader.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "file = pq.ParquetFile('/home/peyman/Downloads/part-00000-f43ea5ba-b3fe-450a-9478-8803ecab3b4b-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "from copy import deepcopy\n",
    "import sqlite3\n",
    "from gis12_mapinference.spatialfunclib import haversine_distance, point_along_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, node_id, latitude, longitude):\n",
    "        self.id = node_id\n",
    "        self.coordinates = (latitude, longitude)\n",
    "        self.edges = set()\n",
    "        self.neighbors = set()\n",
    "\n",
    "# returns index of the nearest node of an array to a given node.\n",
    "def distance_argmin(nodes, sample_node):\n",
    "    distances = []\n",
    "    for i in range(len(nodes)):\n",
    "        distances.append(haversine_distance(nodes[i], sample_node))\n",
    "    return np.argmin(distances)\n",
    "\n",
    "# stick edges to each other based on the common edges between nodes\n",
    "def stick_edge(point_edge_list):\n",
    "    big_edges = {}\n",
    "    while len(point_edge_list) > 0:\n",
    "        point_key, edge_ids = point_edge_list.popitem()\n",
    "        edge_ids = list(edge_ids)\n",
    "        # for nodes with common edges union the the other nodes have the same shred edge\n",
    "        for edge_id in edge_ids:\n",
    "            for point, edges in point_edge_list.copy().items():\n",
    "                if edge_id in edges:\n",
    "                    _ = point_edge_list.pop(point)\n",
    "                    edge_ids += list(edges.difference(set(edge_ids)))\n",
    "        big_edges[point_key] = set(edge_ids)\n",
    "    return big_edges\n",
    "\n",
    "# generate a dictionary of nodes which has node ids and keys and Node objects as values\n",
    "def generate_node_mapping(coords_geoms):\n",
    "    # first create a dictionary with nodes coordinates as keys and Nodes objects as values\n",
    "    # store the edges which each node is shared between them\n",
    "    nodes_str_mapping = {}\n",
    "    id_counter = 0\n",
    "    for index, item in coords_geoms.iteritems():\n",
    "        for i in range(item.shape[0]):\n",
    "            node_key = f'({item[i][0]},{item[i][1]})'\n",
    "            if node_key not in nodes_str_mapping.keys():\n",
    "                nodes_str_mapping[node_key] = Node(id_counter, *item[i])\n",
    "                nodes_str_mapping[node_key].edges.add(index)\n",
    "                id_counter += 1\n",
    "            else:\n",
    "                nodes_str_mapping[node_key].edges.add(index)\n",
    "\n",
    "    # replace the coordinate keys with node ids and store each node neighbors in it\n",
    "    nodes_mapping = {}\n",
    "    for index, item in coords_geoms.iteritems():\n",
    "        for i in range(item.shape[0]):\n",
    "            node_key = f'({item[i][0]},{item[i][1]})'\n",
    "            node_id = nodes_str_mapping[node_key].id\n",
    "            nodes_mapping[node_id] = nodes_str_mapping[node_key]\n",
    "            nodes_mapping[node_id].neighbors.add(\n",
    "                nodes_str_mapping[f'({item[int(not i)][0]},{item[int(not i)][1]})'].id\n",
    "            )\n",
    "    return nodes_mapping\n",
    "\n",
    "# recursively generate virtual nodes (e.g. holes and marbles) between\n",
    "# each pair of neighboring nodes and store them in a list\n",
    "def recursive_node_generation(nodes_mapping, r, d, init_node_id, nodes, traveled_distance):\n",
    "    init_node = nodes_mapping[init_node_id]\n",
    "    for neighbor_id in init_node.neighbors.copy():\n",
    "        # use haversine distance to consider exact distance on earth in meters and taking\n",
    "        # care of latitude and longitude difference in meters per unit\n",
    "        distance = haversine_distance(init_node.coordinates, nodes_mapping[neighbor_id].coordinates)\n",
    "        # for each neighbor of the initial node check the distance condition and if\n",
    "        # it was bigger than specific distance d break the distance and generate\n",
    "        # virtual nodes in between until passing the distance threshold\n",
    "        if distance > d and traveled_distance + d < r:\n",
    "            node_coordinates = point_along_line(\n",
    "                *init_node.coordinates, *nodes_mapping[neighbor_id].coordinates, d/distance\n",
    "            )\n",
    "            new_id = len(nodes_mapping)\n",
    "            nodes.append(node_coordinates)\n",
    "            nodes_mapping[new_id] = Node(new_id, *node_coordinates)\n",
    "            # remove neighbor node from the initial node neighbors and vice versa,\n",
    "            # but just add the neighbor node to the newly generated node because\n",
    "            # we are recursively breaking the distance between current nodes and\n",
    "            # the unseen nodes not the visited ones\n",
    "            nodes_mapping[new_id].neighbors.add(neighbor_id)\n",
    "            nodes_mapping[init_node_id].neighbors.discard(neighbor_id)\n",
    "            nodes_mapping[neighbor_id].neighbors.discard(init_node_id)\n",
    "            recursive_node_generation(nodes_mapping, r, d, new_id, nodes, traveled_distance + d)\n",
    "        # if the distance between the neighbor node and the initial node\n",
    "        # is smaller than the specific distance d, just add and store\n",
    "        # the neighbor id in the newly added nodes list\n",
    "        elif distance < d and traveled_distance + distance < r:\n",
    "            nodes.append(nodes_mapping[neighbor_id].coordinates)\n",
    "            nodes_mapping[neighbor_id].neighbors.discard(init_node_id)\n",
    "            recursive_node_generation(nodes_mapping, r, d, neighbor_id, nodes, traveled_distance + distance)\n",
    "\n",
    "# generate holes and marbles simultaneously and store them in a dict\n",
    "def generate_nodes(gt_nodes_mapping, pr_nodes_mapping, r, d, check_threshold):\n",
    "    initial_mapping_len = len(gt_nodes_mapping)\n",
    "    num_checked_nodes = 0\n",
    "    # fixme: change the break out condition\n",
    "    new_nodes = {'holes': set(), 'marbles': set()}\n",
    "    while num_checked_nodes / initial_mapping_len < check_threshold:\n",
    "        itr_dict = {'holes': gt_nodes_mapping, 'marbles': pr_nodes_mapping}\n",
    "        temp_gen_nodes = {'holes': [], 'marbles': []}\n",
    "        for key, nodes_mapping in itr_dict.items():\n",
    "            # iteratively choose a new random node as a seed and generate\n",
    "            # holes or marbles close to it limited to an specific distance r\n",
    "            traveled_dist = 0\n",
    "            if key == 'holes':\n",
    "                random_node_id = np.random.choice(np.array(list(nodes_mapping.keys())), 1)[0]\n",
    "                # store the current holes_mapping for the next iteration of creating\n",
    "                # marbles because the holes_mapping get change in each iteration\n",
    "                random_hole_id, last_gt_mappings = random_node_id, deepcopy(nodes_mapping)\n",
    "                num_checked_nodes += len(nodes_mapping[random_node_id].neighbors) + 1\n",
    "            else:\n",
    "                pr_nodes = list(nodes_mapping.values())\n",
    "                pr_nodes_coords = np.array([node.coordinates for node in pr_nodes])\n",
    "                try:\n",
    "                    # find the node closet to the initial hole to put the initial marble just as explained by Biagioni\n",
    "                    random_node_id = pr_nodes[\n",
    "                        distance_argmin(pr_nodes_coords, last_gt_mappings[random_hole_id].coordinates)].id\n",
    "                except:\n",
    "                    print(f'Warnnings!\\nThere are not not enough predicted nodes on map to check'\n",
    "                          f' more at least {check_threshold * 100}% of ground truth nodes.')\n",
    "                    return \"\"\n",
    "            temp_gen_nodes[key].append(nodes_mapping[random_node_id].coordinates)\n",
    "            recursive_node_generation(deepcopy(nodes_mapping), r, d, random_node_id, temp_gen_nodes[key], traveled_dist)\n",
    "            # remove the last random node choice from the nodes_mapping\n",
    "            # to avoid mis-operations and more complexity.\n",
    "            # Also remove its connections to its neighbors\n",
    "            random_nbr_ids = nodes_mapping.pop(random_node_id).neighbors\n",
    "            for neighbor_id in random_nbr_ids:\n",
    "                nodes_mapping[neighbor_id].neighbors.discard(random_node_id)\n",
    "            # union the holes and marbles with the last generated set of them to avoid duplications\n",
    "            new_nodes[key] = new_nodes[key].union(set(temp_gen_nodes[key]))\n",
    "    for key, value in new_nodes.items():\n",
    "        new_nodes[key] = list(value)\n",
    "    return new_nodes\n",
    "\n",
    "# match each hole and marble with their nearest marble and hole\n",
    "def match_hole_marble(generated_nodes, error_threshold):\n",
    "    MAX_VALUE = 1e05\n",
    "    distances = np.zeros((len(generated_nodes['marbles']), len(generated_nodes['holes'])))\n",
    "    # compute the haversine distance between each hole and marble\n",
    "    for i in range(distances.shape[0]):\n",
    "        for j in range(distances.shape[1]):\n",
    "            distances[i, j] = haversine_distance(generated_nodes['marbles'][i], generated_nodes['holes'][j])\n",
    "    matches = []\n",
    "    for i in range(distances.shape[0]):\n",
    "        nearest_hole_index = np.argmin(distances[i])\n",
    "        for j in range(distances.shape[1]):\n",
    "            nearest_marble_index = np.argmin(distances[:, j])\n",
    "            # check if just both the hole and the marble are the closets ones to each other,\n",
    "            # add them to the matched pairs not just one of the conditions satisfied\n",
    "            if nearest_hole_index == j and nearest_marble_index == i:\n",
    "                if distances[i, j] <= error_threshold:\n",
    "                    matches.append((i, j))\n",
    "                # fill the matched rows and columns with a big number\n",
    "                # to take them out of consideration for the other comparisons\n",
    "                distances[i] = MAX_VALUE\n",
    "                distances[:, j] = MAX_VALUE\n",
    "    return matches, distances\n",
    "\n",
    "# evaluate precision, recall, f_score\n",
    "def evalute_prf(generated_nodes, error_threshold):\n",
    "    matches, _ = match_hole_marble(generated_nodes, error_threshold)\n",
    "    precision = len(matches) / len(generated_nodes['marbles'])\n",
    "    recall = len(matches) / len(generated_nodes['holes'])\n",
    "    f_score = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6466512702078522 0.16659229510635132 0.2649319929036073\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ground_truth_path = '/home/peyman/Documents/projects/balad/codes/ground-map/map/all_edges.shp'\n",
    "    ground_map = gp.read_file(ground_truth_path)\n",
    "    gt_edges = ground_map['geometry'].apply(lambda x: np.array(x.coords)[:, ::-1])\n",
    "    con = sqlite3.connect(\"gis12_mapinference/skeleton_maps/skeleton_map_1m.db\")\n",
    "    edges_df = pd.read_sql_query(\"SELECT id, in_node, out_node, weight FROM edges\", con)\n",
    "    nodes_df = pd.read_sql_query(\"SELECT id, latitude, longitude, weight FROM nodes\", con)\n",
    "    nodes_df.set_index('id', drop=True, inplace=True)\n",
    "    # find the start and end nodes coordinates for each edge of the edges table\n",
    "    # based on nodes of the nodes table\n",
    "    pr_edges = edges_df.apply(lambda x: np.array([\n",
    "        [nodes_df.loc[x.in_node].latitude, nodes_df.loc[x.in_node].longitude],\n",
    "        [nodes_df.loc[x.out_node].latitude, nodes_df.loc[x.out_node].longitude]\n",
    "    ]), axis=1)\n",
    "    r, d, check_threshold, error_threshold = 1000, 100, 0.15, 50\n",
    "    gt_nodes_dict = generate_node_mapping(gt_edges)\n",
    "    pr_nodes_dict = generate_node_mapping(pr_edges)\n",
    "    new_gen_nodes = generate_nodes(gt_nodes_dict, pr_nodes_dict, r, d, check_threshold)\n",
    "    precision, recall, f_score = evalute_prf(new_gen_nodes, error_threshold)\n",
    "    print(precision, recall, f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7408071748878924 0.12266112266112267 0.21047267167792075\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ground_truth_path = '/home/peyman/Documents/projects/balad/codes/ground-map/map/all_edges.shp'\n",
    "    ground_map = gp.read_file(ground_truth_path)\n",
    "    gt_edges = ground_map['geometry'].apply(lambda x: np.array(x.coords)[:, ::-1])\n",
    "    con = sqlite3.connect(\"gis12_mapinference/skeleton_maps/skeleton_map_1m_mm1.db\")\n",
    "    edges_df = pd.read_sql_query(\"SELECT id, in_node, out_node, weight FROM edges\", con)\n",
    "    nodes_df = pd.read_sql_query(\"SELECT id, latitude, longitude, weight FROM nodes\", con)\n",
    "    nodes_df.set_index('id', drop=True, inplace=True)\n",
    "    # find the start and end nodes coordinates for each edge of the edges table\n",
    "    # based on nodes of the nodes table\n",
    "    pr_edges = edges_df.apply(lambda x: np.array([\n",
    "        [nodes_df.loc[x.in_node].latitude, nodes_df.loc[x.in_node].longitude],\n",
    "        [nodes_df.loc[x.out_node].latitude, nodes_df.loc[x.out_node].longitude]\n",
    "    ]), axis=1)\n",
    "    r, d, check_threshold, error_threshold = 1000, 100, 0.15, 50\n",
    "    gt_nodes_dict = generate_node_mapping(gt_edges)\n",
    "    pr_nodes_dict = generate_node_mapping(pr_edges)\n",
    "    new_gen_nodes = generate_nodes(gt_nodes_dict, pr_nodes_dict, r, d, check_threshold)\n",
    "    precision, recall, f_score = evalute_prf(new_gen_nodes, error_threshold)\n",
    "    print(precision, recall, f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7376744186046512 0.1180762358546754 0.2035682197407265\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ground_truth_path = '/home/peyman/Documents/projects/balad/codes/ground-map/map/all_edges.shp'\n",
    "    ground_map = gp.read_file(ground_truth_path)\n",
    "    gt_edges = ground_map['geometry'].apply(lambda x: np.array(x.coords)[:, ::-1])\n",
    "    con = sqlite3.connect(\"gis12_mapinference/skeleton_maps/skeleton_map_1m_mm2.db\")\n",
    "    edges_df = pd.read_sql_query(\"SELECT id, in_node, out_node, weight FROM edges\", con)\n",
    "    nodes_df = pd.read_sql_query(\"SELECT id, latitude, longitude, weight FROM nodes\", con)\n",
    "    nodes_df.set_index('id', drop=True, inplace=True)\n",
    "    # find the start and end nodes coordinates for each edge of the edges table\n",
    "    # based on nodes of the nodes table\n",
    "    pr_edges = edges_df.apply(lambda x: np.array([\n",
    "        [nodes_df.loc[x.in_node].latitude, nodes_df.loc[x.in_node].longitude],\n",
    "        [nodes_df.loc[x.out_node].latitude, nodes_df.loc[x.out_node].longitude]\n",
    "    ]), axis=1)\n",
    "    r, d, check_threshold, error_threshold = 1000, 100, 0.15, 50\n",
    "    gt_nodes_dict = generate_node_mapping(gt_edges)\n",
    "    pr_nodes_dict = generate_node_mapping(pr_edges)\n",
    "    new_gen_nodes = generate_nodes(gt_nodes_dict, pr_nodes_dict, r, d, check_threshold)\n",
    "    precision, recall, f_score = evalute_prf(new_gen_nodes, error_threshold)\n",
    "    print(precision, recall, f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
